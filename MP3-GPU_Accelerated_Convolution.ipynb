{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU-Accelerated Computation using 1D, 2D, and 3D CUDA Kernels\n",
    "-------------------\n",
    "## Contents\n",
    "- **Getting to know your GPU and the CUDA programming model [not graded]**\n",
    "    - CUDA threads, blocks, and grids, host (CPU) to device (GPU) data movement\n",
    "    - Writing CUDA kernels\n",
    "- **1D CUDA Kernels: Vector Addition and 1D Convolution [35 points]**\n",
    "    - 1D vector addition\n",
    "        - Simple CUDA [10]\n",
    "    - 1D convolution\n",
    "        - Simple CUDA [10]\n",
    "        - Optimized CUDA [15]\n",
    "- **2D CUDA Kernels: Matrix Multiplication [20 points]**\n",
    "    - 2D matrix multiplication\n",
    "        - Simple CUDA [5]\n",
    "        - Optimized CUDA [10]\n",
    "        - Effect of matrix size on performance [5]\n",
    "- **3D CUDA Kernels: Richardson-Lucy Algorithm for 3D Brain Image Deconvolution [45 points]**\n",
    "    - 3D Richardson-Lucy\n",
    "        - Simple CUDA [15]\n",
    "        - Optimized CUDA [20]\n",
    "        - Execute RL, compare, and visualize [10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "# Getting to know your GPU and the CUDA programming model [not graded]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the GPU allocated to us using the ```nvidia-smi``` shell command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "cuda.detect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Knowing the hardware specification (number of streaming multiprocessors (SMs) and number of cores per SM) helps in choosing a good configuration for running user-defined CUDA kernels. Although selecting an optimal execution configuration needs some analysis, some basic rules are:\n",
    "- **The number of blocks in the grid should be larger than the number of SMs on the GPU, typically 2 to 4 times larger.**\n",
    "- **The number of threads per block should be a multiple of 32, typically between 128 and 512.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA lets you configure the matrix of GPU threads to better fit your task. There is no default value to blockdim and griddim, as the optimal settings is dramatically different for different tasks. In addition to resources posted on Canvas, you can study the ones below:\n",
    "\n",
    "**CUDA basics:** Watch [this tutorial](https://www.youtube.com/watch?v=kzXjRFL-gjo) (25:25) about CUDA block & grid model.\n",
    "\n",
    "**Writing CUDA kernels:** https://numba.readthedocs.io/en/stable/cuda/kernels.html. Throughout this MP, keep in mind that CUDA kernels are JIT'ed so the first execution will include numba optimization, compilation, and caching time. To see the benefits of using CUDA kernels, you should time their second runs and not the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit, cuda, vectorize, guvectorize, stencil\n",
    "from numba import prange\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import convolve as scipy_convolve\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "# [Section 1] 1D CUDA Kernels: Vector Addition and 1D Convolution [35 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional vector addition [not graded]\n",
    "\n",
    "The cell below shows the implementations of vector_addition in native Python (numpy) and cuda numba. For demonstration of what's going on, we will implement the vector addition from scratch by iterating over the two vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Vector Addition](./resources/vector_addition.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_addition(vector_A:np.ndarray, vector_B:np.ndarray):\n",
    "    assert vector_A.shape == vector_B.shape\n",
    "    result = np.empty_like(vector_A)\n",
    "    for i in range(len(vector_A)):\n",
    "        result[i] = vector_A[i] + vector_B[i]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>\n",
    "Notice that: \n",
    "\n",
    "1. There are repeated computational operations on different input data. In this case, the operation is the addition of two numbers at each location of the two vectors.\n",
    "2. The repeated computations are independent: the result of the addition of the two numbers from ```vector_A``` and ```vector_B``` at location ```i``` depends on nothing but ```vector_A[i]``` and ```vector_B[i]```. \n",
    "</strong>\n",
    "\n",
    "This means vector addition has the potential to be parallelized by having a computing unit process each addition at the same time. In this MP, you will keep identifying such a pattern that lends itself to parallelization. We can \"unroll\" or \"flatten\" the for loop to have each iteration of the loop running parallely at the same time. The key is assigning each iteration ```i``` correctly to a GPU thread. We will see how this is done in the following GPU implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Vector Addition](./resources/vector_addition_parallel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Exercise] GPU-Accelerated 1D Vector Addition - Simple CUDA [10 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "def vector_addition_cuda(...):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "vector_A = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "vector_B = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "\n",
    "# ---------------------------------------\n",
    "start_time = time.time()\n",
    "print(\"--- [Serial] Starting the timer ---\")\n",
    "serial_result = vector_addition(vector_A, vector_B)\n",
    "print(\"Serial result: \", result)\n",
    "print(\"--- Done: The execution took %s seconds ---\" % (time.time() - start_time))\n",
    "# ---------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "num_blocks = ...\n",
    "num_threads_per_block = ...\n",
    "\n",
    "# run and time CUDA kernel\n",
    "\n",
    "# compare result with serial\n",
    "assert ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice a surprising difference between serial and CUDA-based vector addition. We will see how this performance difference changes with the size of data later in the MP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Exercise] 1D Convolution - Simple CUDA [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "def conv1d_cuda(...):\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Exercise] 1D Convolution - Optimized CUDA (Using CUDA shared and constant memory) [15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, refer to Prof. Steven Lumetta's slides from the 10/31/22 lecture. He shows us how to take 1D convolutions to the limit of the hardware architecture by making use of tiles, shared memory for data re-use, and constant memory for storing the fixed convolution kernel. His version of the 1D convolution is substantially different from the simple CUDA version: \n",
    "- tiled 1D convolution to reuse data loaded into shared memory from global memory (slide 17-23)\n",
    "- convolution filter goes into constant memory (slide 43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "def faster_conv1d_cuda(...):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from numba import cuda, float32\n",
    "\n",
    "# create random data (vector_A) and conv filter (vector_B)\n",
    "vector_A = ...\n",
    "vector_B = ...\n",
    "\n",
    "print(\"A:\", vector_A)\n",
    "print(\"B:\", vector_B)\n",
    "\n",
    "scipy_result = ...\n",
    "print(\"scipy:\", scipy_result)\n",
    "\n",
    "# configure blocks/threads\n",
    "\n",
    "# run and time: 1) scipy, 2) simple CUDA, and 3) optimized CUDA kernels\n",
    "\n",
    "# make sure scipy, simple CUDA, and optimized CUDA all give the same result\n",
    "assert ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: What are your observations from the outputs above?**\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "# [Section 2] 2D CUDA Kernels: Matrix Multiplication [20 points]\n",
    "\n",
    "This example was covered in Lab 6. If needed, you can use the following updated code snippets for this section - https://gist.github.com/neerajwagh/40167f82681ea89d65052e2c1bf20f0c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![2-D Matrix Multiplication](./resources/matmul.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def matmul_serial(mat_A, mat_B):\n",
    "\n",
    "    assert mat_A.shape[1] == mat_B.shape[0]\n",
    "    \n",
    "    output_shape = (mat_A.shape[0], mat_B.shape[1])\n",
    "    result = np.zeros(output_shape)\n",
    "\n",
    "    for i in range(output_shape[0]):\n",
    "        for j in range(output_shape[1]):\n",
    "            for k in range(mat_B.shape[0]):\n",
    "\n",
    "                # resulted matrix\n",
    "                result[i][j] += mat_A[i][k] * mat_B[k][j]\n",
    "\n",
    "    return result\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Exercise] 2D Matrix Multiplication - Simple CUDA [5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "def matmul_cuda(...):\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Exercise] 2D Matrix Multiplication - Optimized CUDA [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "def faster_matmul_cuda(...):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random square matrices of fixed dimension, say 100 \n",
    "fixed_dim = 100\n",
    "matrix_A = ...\n",
    "matrix_B = ...\n",
    "\n",
    "numpy_result = np.matmul(matrix_A, matrix_B)\n",
    "\n",
    "# run and time: 1) numpy, 2) simple CUDA, and 3) optimized CUDA kernels\n",
    "\n",
    "# make sure numpy, simple CUDA, and optimized CUDA all give the same result\n",
    "assert ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Exercise] Effect of matrix size on performance [5]\n",
    "\n",
    "Let's see how the performance of the 2D matrix multiplication kernel is affected by increasing sizes of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "for size in [32, 128, 256, 512, 1024, 2048, 4096]:\n",
    "\n",
    "    matrix_A = ...\n",
    "    matrix_B = ...\n",
    "\n",
    "    if size < 512:\n",
    "        # ---------------------------------------\n",
    "        # time serial \n",
    "        # ---------------------------------------\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # time np.matmul()\n",
    "    # ---------------------------------------    \n",
    "        \n",
    "    # ---------------------------------------\n",
    "    # time simple and optimized CUDA\n",
    "    # ---------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: What are your observations from the outputs above? How does serial and np.matmul() compare to the CUDA kernels?**\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Section 3] 3D CUDA Kernels: Richardson-Lucy Algorithm for 3D Brain Image Deconvolution [45 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a 3D CUDA kernel for the 3D convolution operation performed as part of the RL algorithm we've seen in the previous MPs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Exercise] 3D Convolution - Simple CUDA [15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define kernel here\n",
    "...\n",
    "def convolve(...):\n",
    "    # single channel: grayscale only\n",
    "    ...\n",
    "    return\n",
    "\n",
    "def convolve3D_gpu(matrix, mask):\n",
    "    blockdim = ... # number of threads in each direction\n",
    "    griddim = ... # number of blocks in each direction\n",
    "\n",
    "    matrix_cont = np.ascontiguousarray(matrix, dtype = matrix.dtype)\n",
    "    mask_cont = np.ascontiguousarray(mask, dtype = mask.dtype)\n",
    "\n",
    "    # invoke kernel here\n",
    "    convolve[...](...); ...\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the cell below to test your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.random_sample((240, 240, 155))\n",
    "B = np.random.random_sample((9, 9, 9))\n",
    "\n",
    "# time and check correctness of your kernel against scipy convolve\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Exercise] 3D Convolution - Optimized CUDA [20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define kernel here\n",
    "...\n",
    "def convolve_optim(...):\n",
    "    # single channel: grayscale only\n",
    "    ...\n",
    "    return\n",
    "\n",
    "def convolve3D_gpu_optim(matrix, mask):\n",
    "    blockdim = ... # number of threads in each direction\n",
    "    griddim = ... # number of blocks in each direction\n",
    "\n",
    "    matrix_cont = np.ascontiguousarray(matrix, dtype = matrix.dtype)\n",
    "    mask_cont = np.ascontiguousarray(mask, dtype = mask.dtype)\n",
    "\n",
    "    # invoke kernel here\n",
    "    convolve_optim[...](...); ...\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.random_sample((240, 240, 155))\n",
    "B = np.random.random_sample((9, 9, 9))\n",
    "\n",
    "# time and check correctness of your kernel against scipy convolve\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Exercise] Setup the RL Algorithm [3]\n",
    "\n",
    "Use the 3D brain MRI scan downloaded below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/vb100/Visualize-3D-MRI-Scans-Brain-case/raw/master/data/images/BRATS_001.nii.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "image_path = \"BRATS_001.nii.gz\"\n",
    "image_obj = nib.load(image_path)\n",
    "image_data = image_obj.get_fdata()\n",
    "type(image_data)\n",
    "print(image_data.shape)\n",
    "image_data_by_channel = np.array([image_data[:, :, :, i] for i in range(4)])\n",
    "print(image_data_by_channel.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the 3D RL function from previous MPs below, and drop in the 3D convolution CUDA kernel written above. You will need to two versions - one with simple CUDA and the other with the optimized CUDA kernel.\n",
    "\n",
    "The following code was released on Canvas earlier - https://gist.github.com/neerajwagh/e438bd7e492fd2bfa4cc28325a73284a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the CUDA kernels instead of scipy convolutions here\n",
    "def richardson_lucy_3d(...):\n",
    "    ...\n",
    "    return im_deconv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can optionally downsample the image if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import zoom\n",
    "RESIZE = False\n",
    "if RESIZE:\n",
    "    image_data_by_channel = np.array(\n",
    "        [zoom(channel, (0.7, 0.7, 0.7)) for channel in image_data_by_channel])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a noisy image with a fixed/known PSF that we can then deblur using RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "\n",
    "psf = np.ones((5, 5, 5)) / 125\n",
    "\n",
    "convolved_by_channel = [scipy_convolve(\n",
    "    channel_slice, psf, mode = \"same\") for channel_slice in image_data_by_channel]\n",
    "\n",
    "noisy_by_channel = convolved_by_channel.copy()\n",
    "\n",
    "noisy_by_channel = [channel_slice + (rng.poisson(lam=125, size=channel_slice.shape) - 10 / 255)\n",
    "                    for channel_slice in noisy_by_channel]\n",
    "\n",
    "print(noisy_by_channel[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Exercise] Compare execution time: 1) skimage RL, 2) simple CUDA kernel, and 3) optimized CUDA kernel [4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ITERATIONS_RL = ...\n",
    "\n",
    "# skimage version can be found at https://scikit-image.org/docs/stable/api/skimage.restoration.html#skimage.restoration.richardson_lucy \n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## [Exercise] Visualization - Compare the blurred image and deconvolved image for 1) skimage RL, 2) simple CUDA kernel, and 3) optimized CUDA kernel [3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note - this should be a 3x2 panel plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "353902f3f2f769574ee6d5e609f500cb3c8385ac61494244183cc0b6ad3e28b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
