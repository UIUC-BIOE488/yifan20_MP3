{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU-Accelerated Computation using 1D, 2D, and 3D CUDA Kernels\n",
    "-------------------\n",
    "## Contents\n",
    "- **Getting to know your GPU and the CUDA programming model [not graded]**\n",
    "    - CUDA threads, blocks, and grids, host (CPU) to device (GPU) data movement\n",
    "    - Writing CUDA kernels\n",
    "- **1D CUDA Kernels: Vector Addition and 1D Convolution [35 points]**\n",
    "    - 1D vector addition\n",
    "        - Simple CUDA [10]\n",
    "    - 1D convolution\n",
    "        - Simple CUDA [10]\n",
    "        - Optimized CUDA [15]\n",
    "- **2D CUDA Kernels: Matrix Multiplication [20 points]**\n",
    "    - 2D matrix multiplication\n",
    "        - Simple CUDA [5]\n",
    "        - Optimized CUDA [10]\n",
    "        - Effect of matrix size on performance [5]\n",
    "- **3D CUDA Kernels: Richardson-Lucy Algorithm for 3D Brain Image Deconvolution [45 points]**\n",
    "    - 3D Richardson-Lucy\n",
    "        - Simple CUDA [15]\n",
    "        - Optimized CUDA [20]\n",
    "        - Execute RL, compare, and visualize [10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "# Getting to know your GPU and the CUDA programming model [not graded]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the GPU allocated to us using the ```nvidia-smi``` shell command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 16 21:36:48 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.107.02             Driver Version: 550.107.02     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A6000               On  |   00000000:01:00.0 Off |                  Off |\n",
      "| 30%   25C    P8             29W /  300W |     606MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000               On  |   00000000:23:00.0 Off |                  Off |\n",
      "| 30%   22C    P8             28W /  300W |      13MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA RTX A6000               On  |   00000000:41:00.0 Off |                  Off |\n",
      "| 30%   22C    P8             22W /  300W |      13MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA RTX A6000               On  |   00000000:61:00.0 Off |                  Off |\n",
      "| 30%   21C    P8             19W /  300W |      13MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  NVIDIA RTX A6000               On  |   00000000:81:00.0 Off |                  Off |\n",
      "| 30%   23C    P8             25W /  300W |      13MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  NVIDIA RTX A6000               On  |   00000000:A1:00.0 Off |                  Off |\n",
      "| 30%   23C    P8             19W /  300W |      13MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  NVIDIA RTX A6000               On  |   00000000:C1:00.0 Off |                  Off |\n",
      "| 30%   24C    P8             28W /  300W |      13MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   7  NVIDIA RTX A6000               On  |   00000000:E1:00.0 Off |                  Off |\n",
      "| 30%   24C    P8             27W /  300W |      13MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      3179      G   /usr/libexec/Xorg                               4MiB |\n",
      "|    0   N/A  N/A    998033      C   /home/behera3/miniconda3/bin/python           590MiB |\n",
      "|    1   N/A  N/A      3179      G   /usr/libexec/Xorg                               4MiB |\n",
      "|    2   N/A  N/A      3179      G   /usr/libexec/Xorg                               4MiB |\n",
      "|    3   N/A  N/A      3179      G   /usr/libexec/Xorg                               4MiB |\n",
      "|    4   N/A  N/A      3179      G   /usr/libexec/Xorg                               4MiB |\n",
      "|    5   N/A  N/A      3179      G   /usr/libexec/Xorg                               4MiB |\n",
      "|    6   N/A  N/A      3179      G   /usr/libexec/Xorg                               4MiB |\n",
      "|    7   N/A  N/A      3179      G   /usr/libexec/Xorg                               4MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 CUDA devices\n",
      "id 0     b'NVIDIA RTX A6000'                              [SUPPORTED]\n",
      "                      Compute Capability: 8.6\n",
      "                           PCI Device ID: 0\n",
      "                              PCI Bus ID: 1\n",
      "                                    UUID: GPU-658d074b-280b-8c65-cd06-002e3aede7b9\n",
      "                                Watchdog: Enabled\n",
      "             FP32/FP64 Performance Ratio: 32\n",
      "id 1     b'NVIDIA RTX A6000'                              [SUPPORTED]\n",
      "                      Compute Capability: 8.6\n",
      "                           PCI Device ID: 0\n",
      "                              PCI Bus ID: 35\n",
      "                                    UUID: GPU-4a2e9868-a22d-c89a-7590-64a27f559529\n",
      "                                Watchdog: Enabled\n",
      "             FP32/FP64 Performance Ratio: 32\n",
      "id 2     b'NVIDIA RTX A6000'                              [SUPPORTED]\n",
      "                      Compute Capability: 8.6\n",
      "                           PCI Device ID: 0\n",
      "                              PCI Bus ID: 65\n",
      "                                    UUID: GPU-e1fdbc49-fd1e-1394-f3ff-dd78a2920e7a\n",
      "                                Watchdog: Enabled\n",
      "             FP32/FP64 Performance Ratio: 32\n",
      "id 3     b'NVIDIA RTX A6000'                              [SUPPORTED]\n",
      "                      Compute Capability: 8.6\n",
      "                           PCI Device ID: 0\n",
      "                              PCI Bus ID: 97\n",
      "                                    UUID: GPU-421359ea-eb9e-3f20-650c-e3db21c2f149\n",
      "                                Watchdog: Enabled\n",
      "             FP32/FP64 Performance Ratio: 32\n",
      "id 4     b'NVIDIA RTX A6000'                              [SUPPORTED]\n",
      "                      Compute Capability: 8.6\n",
      "                           PCI Device ID: 0\n",
      "                              PCI Bus ID: 129\n",
      "                                    UUID: GPU-4af46217-d298-c54d-d911-50f344a29625\n",
      "                                Watchdog: Enabled\n",
      "             FP32/FP64 Performance Ratio: 32\n",
      "id 5     b'NVIDIA RTX A6000'                              [SUPPORTED]\n",
      "                      Compute Capability: 8.6\n",
      "                           PCI Device ID: 0\n",
      "                              PCI Bus ID: 161\n",
      "                                    UUID: GPU-06d3934b-0b06-6dfa-0953-406390e47634\n",
      "                                Watchdog: Enabled\n",
      "             FP32/FP64 Performance Ratio: 32\n",
      "id 6     b'NVIDIA RTX A6000'                              [SUPPORTED]\n",
      "                      Compute Capability: 8.6\n",
      "                           PCI Device ID: 0\n",
      "                              PCI Bus ID: 193\n",
      "                                    UUID: GPU-15650c41-fd85-adb6-ae4f-68287a6ef5ee\n",
      "                                Watchdog: Enabled\n",
      "             FP32/FP64 Performance Ratio: 32\n",
      "id 7     b'NVIDIA RTX A6000'                              [SUPPORTED]\n",
      "                      Compute Capability: 8.6\n",
      "                           PCI Device ID: 0\n",
      "                              PCI Bus ID: 225\n",
      "                                    UUID: GPU-1b91f3c9-464d-5818-f93b-9f991cd2d5b7\n",
      "                                Watchdog: Enabled\n",
      "             FP32/FP64 Performance Ratio: 32\n",
      "Summary:\n",
      "\t8/8 devices are supported\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numba import cuda\n",
    "cuda.detect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Knowing the hardware specification (number of streaming multiprocessors (SMs) and number of cores per SM) helps in choosing a good configuration for running user-defined CUDA kernels. Although selecting an optimal execution configuration needs some analysis, some basic rules are:\n",
    "- **The number of blocks in the grid should be larger than the number of SMs on the GPU, typically 2 to 4 times larger.**\n",
    "- **The number of threads per block should be a multiple of 32, typically between 128 and 512.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA lets you configure the matrix of GPU threads to better fit your task. There is no default value to blockdim and griddim, as the optimal settings is dramatically different for different tasks. In addition to resources posted on Canvas, you can study the ones below:\n",
    "\n",
    "**CUDA basics:** Watch [this tutorial](https://www.youtube.com/watch?v=kzXjRFL-gjo) (25:25) about CUDA block & grid model.\n",
    "\n",
    "**Writing CUDA kernels:** https://numba.readthedocs.io/en/stable/cuda/kernels.html. Throughout this MP, keep in mind that CUDA kernels are JIT'ed so the first execution will include numba optimization, compilation, and caching time. To see the benefits of using CUDA kernels, you should time their second runs and not the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit, cuda, vectorize, guvectorize, stencil\n",
    "from numba import prange\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import convolve as scipy_convolve\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "# [Section 1] 1D CUDA Kernels: Vector Addition and 1D Convolution [35 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional vector addition [not graded]\n",
    "\n",
    "The cell below shows the implementations of vector_addition in native Python (numpy) and cuda numba. For demonstration of what's going on, we will implement the vector addition from scratch by iterating over the two vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Vector Addition](./resources/vector_addition.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_addition(vector_A:np.ndarray, vector_B:np.ndarray):\n",
    "    assert vector_A.shape == vector_B.shape\n",
    "    result = np.empty_like(vector_A)\n",
    "    for i in range(len(vector_A)):\n",
    "        result[i] = vector_A[i] + vector_B[i]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>\n",
    "Notice that: \n",
    "\n",
    "1. There are repeated computational operations on different input data. In this case, the operation is the addition of two numbers at each location of the two vectors.\n",
    "2. The repeated computations are independent: the result of the addition of the two numbers from ```vector_A``` and ```vector_B``` at location ```i``` depends on nothing but ```vector_A[i]``` and ```vector_B[i]```. \n",
    "</strong>\n",
    "\n",
    "This means vector addition has the potential to be parallelized by having a computing unit process each addition at the same time. In this MP, you will keep identifying such a pattern that lends itself to parallelization. We can \"unroll\" or \"flatten\" the for loop to have each iteration of the loop running parallely at the same time. The key is assigning each iteration ```i``` correctly to a GPU thread. We will see how this is done in the following GPU implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Vector Addition](./resources/vector_addition_parallel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Exercise] GPU-Accelerated 1D Vector Addition - Simple CUDA [10 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "\n",
    "def vector_addition_cuda(a, b, c):\n",
    "    idx = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "    if idx < a.size:\n",
    "        c[idx] = a[idx] + b[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [Serial] Starting the timer ---\n",
      "Serial result:  [ 2  4  6  8 10 12 14 16]\n",
      "--- Done: The execution took 0.0002434253692626953 seconds ---\n",
      "--- [CUDA] Starting the timer ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/BIOE488/FA24/conda/cudabio/lib/python3.12/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 4 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA result: [ 2  4  6  8 10 12 14 16]\n",
      "--- Done: The execution took 0.26332831382751465 seconds ---\n",
      "Test Passed\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "vector_A = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "vector_B = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "\n",
    "# ---------------------------------------\n",
    "start_time = time.time()\n",
    "print(\"--- [Serial] Starting the timer ---\")\n",
    "serial_result = vector_addition(vector_A, vector_B)\n",
    "print(\"Serial result: \", serial_result)\n",
    "print(\"--- Done: The execution took %s seconds ---\" % (time.time() - start_time))\n",
    "# ---------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "num_blocks = (vector_A.size + 1) // 2\n",
    "num_threads_per_block = 2\n",
    "\n",
    "# run and time CUDA kernel\n",
    "A = cuda.to_device(vector_A)\n",
    "B = cuda.to_device(vector_B)\n",
    "C = cuda.device_array_like(vector_A)\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"--- [CUDA] Starting the timer ---\")\n",
    "vector_addition_cuda[num_blocks, num_threads_per_block](A, B, C)\n",
    "cuda.synchronize()\n",
    "cuda_result = C.copy_to_host()\n",
    "print(\"CUDA result:\", cuda_result)\n",
    "print(\"--- Done: The execution took %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# compare result with serial\n",
    "assert np.allclose(serial_result, cuda_result), \"Test Failed\"\n",
    "print(\"Test Passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice a surprising difference between serial and CUDA-based vector addition. We will see how this performance difference changes with the size of data later in the MP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Exercise] 1D Convolution - Simple CUDA [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def conv1d_cuda(input_array, kernel, output):\n",
    "    idx = cuda.grid(1)  \n",
    "    radius = len(kernel) // 2\n",
    "    if idx < len(output):\n",
    "        result = 0.0\n",
    "        for k in range(len(kernel)):\n",
    "            neighbor_idx = idx - radius + k\n",
    "            if 0 <= neighbor_idx < len(input_array):\n",
    "                result += input_array[neighbor_idx] * kernel[k]\n",
    "        output[idx] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Exercise] 1D Convolution - Optimized CUDA (Using CUDA shared and constant memory) [15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, refer to Prof. Steven Lumetta's slides from the 10/31/22 lecture. He shows us how to take 1D convolutions to the limit of the hardware architecture by making use of tiles, shared memory for data re-use, and constant memory for storing the fixed convolution kernel. His version of the 1D convolution is substantially different from the simple CUDA version: \n",
    "- tiled 1D convolution to reuse data loaded into shared memory from global memory (slide 17-23)\n",
    "- convolution filter goes into constant memory (slide 43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def faster_conv1d_cuda(input_data, kernel, output):\n",
    "    shared_input = cuda.shared.array(shape=256, dtype=float32)\n",
    "    \n",
    "    tx = cuda.threadIdx.x\n",
    "    idx = cuda.grid(1)\n",
    "    kernel_radius = len(kernel) // 2\n",
    "    tile_width = cuda.blockDim.x + 2 * kernel_radius\n",
    "    \n",
    "    shared_idx = tx + kernel_radius\n",
    "    if idx < len(input_data):\n",
    "        shared_input[shared_idx] = input_data[idx]\n",
    "    else:\n",
    "        shared_input[shared_idx] = 0.0\n",
    "    \n",
    "    if tx < kernel_radius:\n",
    "        left_idx = idx - kernel_radius\n",
    "        shared_input[tx] = input_data[left_idx] if left_idx >= 0 else 0.0\n",
    "\n",
    "        right_idx = idx + cuda.blockDim.x\n",
    "        shared_input[shared_idx + cuda.blockDim.x] = input_data[right_idx] if right_idx < len(input_data) else 0.0\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    if idx < len(output):\n",
    "        result = 0.0\n",
    "        for k in range(len(kernel)):\n",
    "            result += shared_input[tx + k] * kernel[k]\n",
    "        output[idx] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: [0.2879861  0.8393616  0.5631164  ... 0.99938136 0.6321957  0.27873346]\n",
      "B: [1. 2. 3. 2. 1.]\n",
      "scipy time: 0.00019240379333496094\n",
      "Simple CUDA time: 0.058344125747680664\n",
      "Faster CUDA time: 0.08314085006713867\n",
      "Test Passed\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from numba import cuda, float32\n",
    "from scipy.signal import convolve\n",
    "\n",
    "\n",
    "# create random data (vector_A) and conv filter (vector_B)\n",
    "vector_A = np.random.rand(1024).astype(np.float32)\n",
    "vector_B = np.array([1, 2, 3, 2, 1], dtype=np.float32)\n",
    "\n",
    "print(\"A:\", vector_A)\n",
    "print(\"B:\", vector_B)\n",
    "\n",
    "# Scipy\n",
    "start_time = time.time()\n",
    "scipy_result = convolve(vector_A, vector_B, mode='same')\n",
    "scipy_time = time.time() - start_time\n",
    "print(\"scipy time: %s\" % scipy_time)\n",
    "\n",
    "# Simple\n",
    "threads = 8\n",
    "blocks = (len(vector_A) + threads - 1) // threads\n",
    "A = cuda.to_device(vector_A)\n",
    "B = cuda.to_device(vector_B)\n",
    "C = cuda.device_array_like(vector_A)\n",
    "\n",
    "start_time = time.time()\n",
    "conv1d_cuda[blocks, threads](A, B, C)\n",
    "cuda.synchronize()\n",
    "simple_time = time.time() - start_time\n",
    "simple_result = C.copy_to_host()\n",
    "print(\"Simple CUDA time: % s\" % simple_time)\n",
    "\n",
    "# Fast\n",
    "threads = 8\n",
    "blocks = (len(vector_A) + (threads//2) - 1) // (threads//2)\n",
    "A = cuda.to_device(vector_A)\n",
    "B = cuda.to_device(vector_B)\n",
    "C = cuda.device_array_like(vector_A)\n",
    "\n",
    "start_time = time.time()\n",
    "faster_conv1d_cuda[blocks, threads](A, B, C)\n",
    "cuda.synchronize()\n",
    "faster_time = time.time() - start_time\n",
    "faster_result = C.copy_to_host()\n",
    "print(\"Faster CUDA time: % s\" % faster_time)\n",
    "\n",
    "# make sure scipy, simple CUDA, and optimized CUDA all give the same result\n",
    "assert np.allclose(scipy_result, simple_result, atol=1e-5), \"Simple CUDA Failed\"\n",
    "assert np.allclose(scipy_result, faster_result, atol=1e-5), \"Faster CUDA Failed\"\n",
    "print(\"Test Passed\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: What are your observations from the outputs above?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Simple CUDA takes more time, which might be caused by the time used to load stuff into gpu and gettting stuff back. Faster CUDA is similar to scipy. I believe when data is scaled up, CUDA models will be faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "# [Section 2] 2D CUDA Kernels: Matrix Multiplication [20 points]\n",
    "\n",
    "This example was covered in Lab 6. If needed, you can use the following updated code snippets for this section - https://gist.github.com/neerajwagh/40167f82681ea89d65052e2c1bf20f0c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![2-D Matrix Multiplication](./resources/matmul.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def matmul_serial(mat_A, mat_B):\n",
    "\n",
    "    assert mat_A.shape[1] == mat_B.shape[0]\n",
    "    \n",
    "    output_shape = (mat_A.shape[0], mat_B.shape[1])\n",
    "    result = np.zeros(output_shape)\n",
    "\n",
    "    for i in range(output_shape[0]):\n",
    "        for j in range(output_shape[1]):\n",
    "            for k in range(mat_B.shape[0]):\n",
    "\n",
    "                # resulted matrix\n",
    "                result[i][j] += mat_A[i][k] * mat_B[k][j]\n",
    "\n",
    "    return result\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Exercise] 2D Matrix Multiplication - Simple CUDA [5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def matmul_cuda(A, B, C):\n",
    "    row, col = cuda.grid(2)\n",
    "    if row < C.shape[0] and col < C.shape[1]:\n",
    "        temp = 0.\n",
    "        for j in range(A.shape[1]):\n",
    "            temp += A[row, j] * B[j, col]\n",
    "        C[row, col] = temp\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Exercise] 2D Matrix Multiplication - Optimized CUDA [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def faster_matmul_cuda(A, B, C):\n",
    "    tile_dim = cuda.blockDim.x\n",
    "    row_idx = cuda.blockIdx.y * tile_dim + cuda.threadIdx.y\n",
    "    col_idx = cuda.blockIdx.x * tile_dim + cuda.threadIdx.x\n",
    "\n",
    "    A_shared = cuda.shared.array(shape=(32, 32), dtype=float32)\n",
    "    B_shared = cuda.shared.array(shape=(32, 32), dtype=float32)\n",
    "    result = 0.0\n",
    "    total_tiles = (A.shape[1] + tile_dim - 1) // tile_dim\n",
    "\n",
    "    for tile_idx in range(total_tiles):\n",
    "        A_col_idx = tile_idx * tile_dim + cuda.threadIdx.x\n",
    "        B_row_idx = tile_idx * tile_dim + cuda.threadIdx.y\n",
    "\n",
    "        if row_idx < A.shape[0] and A_col_idx < A.shape[1]:\n",
    "            A_shared[cuda.threadIdx.y, cuda.threadIdx.x] = A[row_idx, A_col_idx]\n",
    "        else:\n",
    "            A_shared[cuda.threadIdx.y, cuda.threadIdx.x] = 0.0\n",
    "        if B_row_idx < B.shape[0] and col_idx < B.shape[1]:\n",
    "            B_shared[cuda.threadIdx.y, cuda.threadIdx.x] = B[B_row_idx, col_idx]\n",
    "        else:\n",
    "            B_shared[cuda.threadIdx.y, cuda.threadIdx.x] = 0.0\n",
    "        cuda.syncthreads()\n",
    "        for k in range(tile_dim):\n",
    "            result += A_shared[cuda.threadIdx.y, k] * B_shared[k, cuda.threadIdx.x]\n",
    "        cuda.syncthreads()\n",
    "\n",
    "    if row_idx < C.shape[0] and col_idx < C.shape[1]:\n",
    "        C[row_idx, col_idx] = result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy 9.322166442871094e-05 seconds\n",
      "Simple CUDA time: 0.05314040184020996\n",
      "Faster CUDA time: 0.11980962753295898\n",
      "Test Passed\n"
     ]
    }
   ],
   "source": [
    "# create random square matrices of fixed dimension, say 100 \n",
    "fixed_dim = 100\n",
    "matrix_A = np.random.rand(fixed_dim, fixed_dim)\n",
    "matrix_B = np.random.rand(fixed_dim, fixed_dim)\n",
    "\n",
    "numpy_result = np.matmul(matrix_A, matrix_B)\n",
    "\n",
    "# numpy\n",
    "start_time = time.time()\n",
    "numpy_result = np.matmul(matrix_A, matrix_B)\n",
    "print(\"numpy %s seconds\" % (time.time() - start_time))\n",
    "\n",
    "threads = 8\n",
    "block_size = (fixed_dim + threads - 1) // threads\n",
    "blocks = (block_size, block_size)\n",
    "\n",
    "# Simple\n",
    "\n",
    "A = cuda.to_device(matrix_A)\n",
    "B = cuda.to_device(matrix_B)\n",
    "C = cuda.device_array((fixed_dim, fixed_dim), dtype=np.float32)\n",
    "start_time = time.time()\n",
    "matmul_cuda[blocks, (threads, threads)](A, B, C)\n",
    "cuda.synchronize()\n",
    "simple_time = time.time() - start_time\n",
    "simple_result = C.copy_to_host()\n",
    "print(\"Simple CUDA time: % s\" % simple_time)\n",
    "\n",
    "# Faster\n",
    "A = cuda.to_device(matrix_A)\n",
    "B = cuda.to_device(matrix_B)\n",
    "C = cuda.device_array((fixed_dim, fixed_dim), dtype=np.float32)\n",
    "start_time = time.time()\n",
    "faster_matmul_cuda[blocks, (threads, threads)](A, B, C)\n",
    "cuda.synchronize()\n",
    "faster_time = time.time() - start_time\n",
    "faster_result = C.copy_to_host()\n",
    "print(\"Faster CUDA time: % s\" % faster_time)\n",
    "\n",
    "# make sure numpy, simple CUDA, and optimized CUDA all give the same result\n",
    "assert np.allclose(numpy_result, simple_result, atol=1e-5), \"Simple CUDA failed\"\n",
    "assert np.allclose(numpy_result, faster_result, atol=1e-5), \"Optimized CUDA failed\"\n",
    "print(\"Test Passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Exercise] Effect of matrix size on performance [5]\n",
    "\n",
    "Let's see how the performance of the 2D matrix multiplication kernel is affected by increasing sizes of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "Numpy: 0.0001780986785888672 s\n",
      "Simple : 0.00021147727966308594 s\n",
      "Faster: 8.273124694824219e-05 s \n",
      "\n",
      "128\n",
      "Numpy: 0.0003590583801269531 s\n",
      "Simple : 0.00012922286987304688 s\n",
      "Faster: 0.0001201629638671875 s \n",
      "\n",
      "256\n",
      "Numpy: 0.032785892486572266 s\n",
      "Simple : 0.00023102760314941406 s\n",
      "Faster: 0.000247955322265625 s \n",
      "\n",
      "512\n",
      "Simple : 0.0010972023010253906 s\n",
      "Faster: 0.0014286041259765625 s \n",
      "\n",
      "1024\n",
      "Simple : 0.007730007171630859 s\n",
      "Faster: 0.008681774139404297 s \n",
      "\n",
      "2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/BIOE488/FA24/conda/cudabio/lib/python3.12/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 4 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/shared/BIOE488/FA24/conda/cudabio/lib/python3.12/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 64 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple : 0.0608830451965332 s\n",
      "Faster: 0.06542181968688965 s \n",
      "\n",
      "4096\n",
      "Simple : 0.45159029960632324 s\n",
      "Faster: 0.5144104957580566 s \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for size in [32, 128, 256, 512, 1024, 2048, 4096]:\n",
    "    print(size)\n",
    "    matrix_A = np.random.rand(size, size)\n",
    "    matrix_B = np.random.rand(size, size)\n",
    "\n",
    "    if size < 512:\n",
    "        start_time = time.time()\n",
    "        numpy_result = np.matmul(matrix_A, matrix_B)\n",
    "        numpy_time = time.time() - start_time\n",
    "        print(f\"Numpy: {numpy_time} s\")\n",
    "\n",
    "\n",
    "    threads = 16\n",
    "    block_size = (size + threads - 1) // threads\n",
    "    blocks = (block_size, block_size)\n",
    "\n",
    "    A = cuda.to_device(matrix_A)\n",
    "    B = cuda.to_device(matrix_B)\n",
    "    C = cuda.device_array((size, size), dtype=np.float32)\n",
    "\n",
    "    start_time = time.time()\n",
    "    matmul_cuda[blocks, (threads, threads)](A, B, C)\n",
    "    cuda.synchronize()\n",
    "    simple_time = time.time() - start_time\n",
    "    simple_result = C.copy_to_host()\n",
    "    print(f\"Simple : {simple_time} s\")\n",
    "\n",
    "    A = cuda.to_device(matrix_A)\n",
    "    B = cuda.to_device(matrix_B)\n",
    "    C = cuda.device_array((size, size), dtype=np.float32)\n",
    "    start_time = time.time()\n",
    "    faster_matmul_cuda[blocks, (threads, threads)](A, B, C)\n",
    "    cuda.synchronize()\n",
    "    faster_time = time.time() - start_time\n",
    "    faster_result = C.copy_to_host()\n",
    "    print(f\"Faster: {faster_time} s \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: What are your observations from the outputs above? How does serial and np.matmul() compare to the CUDA kernels?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "The serial one is much slower than cuda ones. When sizes are small, faster cuda is faster, but when the scales go up, the difference is diminished.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Section 3] 3D CUDA Kernels: Richardson-Lucy Algorithm for 3D Brain Image Deconvolution [45 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a 3D CUDA kernel for the 3D convolution operation performed as part of the RL algorithm we've seen in the previous MPs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Exercise] 3D Convolution - Simple CUDA [15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define kernel here\n",
    "...\n",
    "@cuda.jit\n",
    "def convolve(matrix, mask, result):\n",
    "    x, y, z = cuda.grid(3)\n",
    "    if x >= result.shape[0] or y >= result.shape[1] or z >= result.shape[2]:\n",
    "        return\n",
    "    center_x = (mask.shape[0] // 2)\n",
    "    center_y = (mask.shape[1] // 2)\n",
    "    center_z = (mask.shape[2] // 2)\n",
    "    max_x = mask.shape[0] - 1\n",
    "    max_y = mask.shape[1] - 1\n",
    "    max_z = mask.shape[2] - 1\n",
    "    value_sum = 0.0\n",
    "    for offset_x in range(mask.shape[0]):\n",
    "        src_x = x - center_x + offset_x\n",
    "        if 0 <= src_x < result.shape[0]:\n",
    "            for offset_y in range(mask.shape[1]):\n",
    "                src_y = y - center_y + offset_y\n",
    "                if 0 <= src_y < result.shape[1]:\n",
    "                    for offset_z in range(mask.shape[2]):\n",
    "                        src_z = z - center_z + offset_z\n",
    "                        if 0 <= src_z < result.shape[2]:\n",
    "                            value_sum += (\n",
    "                                matrix[src_x, src_y, src_z] \n",
    "                                * mask[max_x - offset_x, max_y - offset_y, max_z - offset_z])\n",
    "    result[x, y, z] = value_sum\n",
    "    return\n",
    "\n",
    "def convolve3D_gpu(matrix, mask):\n",
    "    blockdim = 1\n",
    "    griddim = (256,256,256) # blocks in each direction\n",
    "    \n",
    "    matrix_cont = np.ascontiguousarray(matrix, dtype = matrix.dtype)\n",
    "    mask_cont = np.ascontiguousarray(mask, dtype = mask.dtype)\n",
    "    result = np.ascontiguousarray(np.zeros(matrix.shape), dtype=matrix.dtype)\n",
    "    # invoke kernel here\n",
    "    convolve[griddim, blockdim](matrix_cont, mask_cont, result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the cell below to test your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scipy: 0.29083728790283203 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/BIOE488/FA24/conda/cudabio/lib/python3.12/site-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: 1.0505082607269287 s\n",
      "Test Passed\n"
     ]
    }
   ],
   "source": [
    "A = np.random.random_sample((240, 240, 155))\n",
    "B = np.random.random_sample((9, 9, 9))\n",
    "\n",
    "# time and check correctness of your kernel against scipy convolve\n",
    "start_time = time.time()\n",
    "scipy_result = scipy_convolve(A, B, mode='same')\n",
    "scipy_time = time.time() - start_time\n",
    "print(f\"Scipy: {scipy_time} s\")\n",
    "\n",
    "start_time = time.time()\n",
    "cuda_result = convolve3D_gpu(A, B)\n",
    "cuda_time = time.time() - start_time\n",
    "print(f\"CUDA: {cuda_time:} s\")\n",
    "\n",
    "assert np.allclose(scipy_result, cuda_result, atol=1e-5), \"Test Failed\"\n",
    "print(\"Test Passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Exercise] 3D Convolution - Optimized CUDA [20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda, float32\n",
    "\n",
    "@cuda.jit\n",
    "def convolve_optim(matrix, mask, result):\n",
    "    # Block and thread dimensions\n",
    "    bSizex, bSizey, bSizez = 8, 8, 16\n",
    "    \n",
    "    # Block and thread indices\n",
    "    bx, by, bz = cuda.blockIdx.x, cuda.blockIdx.y, cuda.blockIdx.z\n",
    "    tx, ty, tz = cuda.threadIdx.x, cuda.threadIdx.y, cuda.threadIdx.z\n",
    "\n",
    "    # Shared memory for tile\n",
    "    tile = cuda.shared.array((bSizex, bSizey, bSizez), dtype=float32)\n",
    "\n",
    "    # Output dimensions for the current block\n",
    "    outDimx = bSizex - mask.shape[0] + 1\n",
    "    outDimy = bSizey - mask.shape[1] + 1\n",
    "    outDimz = bSizez - mask.shape[2] + 1\n",
    "\n",
    "    x = bx * outDimx + tx\n",
    "    y = by * outDimy + ty\n",
    "    z = bz * outDimz + tz\n",
    "    sx = x - mask.shape[0] // 2\n",
    "    sy = y - mask.shape[1] // 2\n",
    "    sz = z - mask.shape[2] // 2\n",
    "\n",
    "    # Populate shared memory tile\n",
    "    if 0 <= sx < matrix.shape[0] and 0 <= sy < matrix.shape[1] and 0 <= sz < matrix.shape[2]:\n",
    "        tile[tx, ty, tz] = matrix[sx, sy, sz]\n",
    "    else:\n",
    "        tile[tx, ty, tz] = 0.0\n",
    "    cuda.syncthreads()\n",
    "\n",
    "    if x < result.shape[0] and y < result.shape[1] and z < result.shape[2]:\n",
    "        if tx < outDimx and ty < outDimy and tz < outDimz:\n",
    "            value = 0.0\n",
    "            for i in range(mask.shape[0]):\n",
    "                for j in range(mask.shape[1]):\n",
    "                    for k in range(mask.shape[2]):\n",
    "                        value += tile[tx + i, ty + j, tz + k] * mask[mask.shape[0] - 1 - i, mask.shape[1] - 1 - j, mask.shape[2] - 1 - k]\n",
    "            result[x, y, z] = value\n",
    "\n",
    "\n",
    "def convolve3D_gpu_optim(matrix, mask):\n",
    "    blockdim = 1\n",
    "    griddim = (256,256,256)\n",
    "    matrix_cont = np.ascontiguousarray(matrix, dtype = matrix.dtype)\n",
    "    mask_cont = np.ascontiguousarray(mask, dtype = mask.dtype)\n",
    "    result = np.ascontiguousarray(np.zeros(matrix_cont.shape), dtype = mask.dtype)\n",
    "    convolve_optim[griddim, blockdim](matrix_cont, mask_cont, result);\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scipy0.2845032215118408 s\n",
      "cuda: 0.9367644786834717 \n",
      "Test passed\n"
     ]
    }
   ],
   "source": [
    "A = np.random.random_sample((240, 240, 155))\n",
    "B = np.random.random_sample((9, 9, 9))\n",
    "\n",
    "# time and check correctness of your kernel against scipy convolve\n",
    "\n",
    "start_time = time.time()\n",
    "scipy_result = scipy_convolve(A, B, mode='same')\n",
    "scipy_time = time.time() - start_time\n",
    "print(f\"Scipy{scipy_time} s\")\n",
    "\n",
    "start_time = time.time()\n",
    "cuda_result = convolve3D_gpu(A, B)\n",
    "cuda_time = time.time() - start_time\n",
    "print(f\"cuda: {cuda_time} \")\n",
    "\n",
    "assert np.allclose(scipy_result, cuda_result, atol=1e-5), \"Test failed\"\n",
    "print(\"Test passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Exercise] Setup the RL Algorithm [3]\n",
    "\n",
    "Use the 3D brain MRI scan downloaded below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/vb100/Visualize-3D-MRI-Scans-Brain-case/raw/master/data/images/BRATS_001.nii.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "image_path = \"BRATS_001.nii.gz\"\n",
    "image_obj = nib.load(image_path)\n",
    "image_data = image_obj.get_fdata()\n",
    "type(image_data)\n",
    "print(image_data.shape)\n",
    "image_data_by_channel = np.array([image_data[:, :, :, i] for i in range(4)])\n",
    "print(image_data_by_channel.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the 3D RL function from previous MPs below, and drop in the 3D convolution CUDA kernel written above. You will need to two versions - one with simple CUDA and the other with the optimized CUDA kernel.\n",
    "\n",
    "The following code was released on Canvas earlier - https://gist.github.com/neerajwagh/e438bd7e492fd2bfa4cc28325a73284a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the CUDA kernels instead of scipy convolutions here\n",
    "def richardson_lucy_3d(...):\n",
    "    ...\n",
    "    return im_deconv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can optionally downsample the image if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import zoom\n",
    "RESIZE = False\n",
    "if RESIZE:\n",
    "    image_data_by_channel = np.array(\n",
    "        [zoom(channel, (0.7, 0.7, 0.7)) for channel in image_data_by_channel])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a noisy image with a fixed/known PSF that we can then deblur using RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "\n",
    "psf = np.ones((5, 5, 5)) / 125\n",
    "\n",
    "convolved_by_channel = [scipy_convolve(\n",
    "    channel_slice, psf, mode = \"same\") for channel_slice in image_data_by_channel]\n",
    "\n",
    "noisy_by_channel = convolved_by_channel.copy()\n",
    "\n",
    "noisy_by_channel = [channel_slice + (rng.poisson(lam=125, size=channel_slice.shape) - 10 / 255)\n",
    "                    for channel_slice in noisy_by_channel]\n",
    "\n",
    "print(noisy_by_channel[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Exercise] Compare execution time: 1) skimage RL, 2) simple CUDA kernel, and 3) optimized CUDA kernel [4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ITERATIONS_RL = ...\n",
    "\n",
    "# skimage version can be found at https://scikit-image.org/docs/stable/api/skimage.restoration.html#skimage.restoration.richardson_lucy \n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## [Exercise] Visualization - Compare the blurred image and deconvolved image for 1) skimage RL, 2) simple CUDA kernel, and 3) optimized CUDA kernel [3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note - this should be a 3x2 panel plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "353902f3f2f769574ee6d5e609f500cb3c8385ac61494244183cc0b6ad3e28b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
